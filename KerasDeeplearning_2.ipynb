{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasDeeplearning-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPvR1daf5erFaIDlXSxNvap",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellodoksa/machine-learning-Project/blob/master/KerasDeeplearning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiVN17-x6Rvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import tensorflow as tf \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERfv0AAUCZaN",
        "colab_type": "text"
      },
      "source": [
        "# 와인 종류 예측하기 :: 중간에 멈추기 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk55hVTi6b2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pre = pd.read_csv('./wine.csv')\n",
        "df= df_pre.sample(frac=1) # 정해진 비율로 랜덤으로 가져와라 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU4zJ7Rv9Uas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = df.values\n",
        "x= dataset[:,0:12]\n",
        "y= dataset[:,12]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p00Z9YxV9kf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense \n",
        "from keras.callbacks import ModelCheckpoint , EarlyStopping\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsdh_6P79rg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUcXe4nu9vUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(30, input_dim =12 , activation ='relu'))\n",
        "model.add(Dense(12,activation = 'relu'))\n",
        "model.add(Dense(8,activation = 'relu'))\n",
        "model.add(Dense(1,activation = 'sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrF4iqTu-DwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 컴파일 \n",
        "model.compile(loss='binary_crossentropy' ,\n",
        "              optimizer ='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjRd1q-a_OQg",
        "colab_type": "code",
        "outputId": "bbea6b91-5fbf-40c9-a183-09e41e2c57fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x,y,epochs=200,batch_size=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "6496/6496 [==============================] - 0s 68us/step - loss: 0.3830 - accuracy: 0.7528\n",
            "Epoch 2/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.3218 - accuracy: 0.8390\n",
            "Epoch 3/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.2863 - accuracy: 0.9153\n",
            "Epoch 4/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.2305 - accuracy: 0.9306\n",
            "Epoch 5/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.1965 - accuracy: 0.9344\n",
            "Epoch 6/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.1878 - accuracy: 0.9370\n",
            "Epoch 7/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.1826 - accuracy: 0.9387\n",
            "Epoch 8/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.1773 - accuracy: 0.9386\n",
            "Epoch 9/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.1641 - accuracy: 0.9438\n",
            "Epoch 10/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.1574 - accuracy: 0.9466\n",
            "Epoch 11/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.1454 - accuracy: 0.9474\n",
            "Epoch 12/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.1258 - accuracy: 0.9547\n",
            "Epoch 13/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.1107 - accuracy: 0.9623\n",
            "Epoch 14/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.1020 - accuracy: 0.9651\n",
            "Epoch 15/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0964 - accuracy: 0.9675\n",
            "Epoch 16/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0935 - accuracy: 0.9680\n",
            "Epoch 17/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0964 - accuracy: 0.9677\n",
            "Epoch 18/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0827 - accuracy: 0.9731\n",
            "Epoch 19/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0854 - accuracy: 0.9700\n",
            "Epoch 20/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0810 - accuracy: 0.9729\n",
            "Epoch 21/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.0763 - accuracy: 0.9757\n",
            "Epoch 22/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0791 - accuracy: 0.9744\n",
            "Epoch 23/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0710 - accuracy: 0.9772\n",
            "Epoch 24/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0691 - accuracy: 0.9783\n",
            "Epoch 25/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0683 - accuracy: 0.9775\n",
            "Epoch 26/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0752 - accuracy: 0.9743\n",
            "Epoch 27/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0675 - accuracy: 0.9781\n",
            "Epoch 28/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0656 - accuracy: 0.9786\n",
            "Epoch 29/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0674 - accuracy: 0.9791\n",
            "Epoch 30/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0623 - accuracy: 0.9815\n",
            "Epoch 31/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0746 - accuracy: 0.9758\n",
            "Epoch 32/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0618 - accuracy: 0.9804\n",
            "Epoch 33/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0634 - accuracy: 0.9792\n",
            "Epoch 34/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0595 - accuracy: 0.9809\n",
            "Epoch 35/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0624 - accuracy: 0.9792\n",
            "Epoch 36/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0660 - accuracy: 0.9791\n",
            "Epoch 37/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0608 - accuracy: 0.9798\n",
            "Epoch 38/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0583 - accuracy: 0.9809\n",
            "Epoch 39/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0563 - accuracy: 0.9814\n",
            "Epoch 40/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.0590 - accuracy: 0.9820\n",
            "Epoch 41/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.0563 - accuracy: 0.9825\n",
            "Epoch 42/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0596 - accuracy: 0.9804\n",
            "Epoch 43/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0645 - accuracy: 0.9794\n",
            "Epoch 44/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0567 - accuracy: 0.9811\n",
            "Epoch 45/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0623 - accuracy: 0.9801\n",
            "Epoch 46/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0594 - accuracy: 0.9817\n",
            "Epoch 47/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0544 - accuracy: 0.9826\n",
            "Epoch 48/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0536 - accuracy: 0.9814\n",
            "Epoch 49/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0552 - accuracy: 0.9826\n",
            "Epoch 50/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0583 - accuracy: 0.9826\n",
            "Epoch 51/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0558 - accuracy: 0.9823\n",
            "Epoch 52/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.0553 - accuracy: 0.9814\n",
            "Epoch 53/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0527 - accuracy: 0.9837\n",
            "Epoch 54/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0543 - accuracy: 0.9823\n",
            "Epoch 55/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0506 - accuracy: 0.9838\n",
            "Epoch 56/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0611 - accuracy: 0.9814\n",
            "Epoch 57/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0581 - accuracy: 0.9821\n",
            "Epoch 58/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0486 - accuracy: 0.9846\n",
            "Epoch 59/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0536 - accuracy: 0.9825\n",
            "Epoch 60/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0523 - accuracy: 0.9840\n",
            "Epoch 61/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0497 - accuracy: 0.9843\n",
            "Epoch 62/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0536 - accuracy: 0.9826\n",
            "Epoch 63/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0628 - accuracy: 0.9798\n",
            "Epoch 64/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0555 - accuracy: 0.9821\n",
            "Epoch 65/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0553 - accuracy: 0.9823\n",
            "Epoch 66/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0496 - accuracy: 0.9845\n",
            "Epoch 67/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0498 - accuracy: 0.9849\n",
            "Epoch 68/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0488 - accuracy: 0.9838\n",
            "Epoch 69/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0502 - accuracy: 0.9848\n",
            "Epoch 70/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0471 - accuracy: 0.9860\n",
            "Epoch 71/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0473 - accuracy: 0.9846\n",
            "Epoch 72/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0510 - accuracy: 0.9838\n",
            "Epoch 73/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0467 - accuracy: 0.9857\n",
            "Epoch 74/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0480 - accuracy: 0.9838\n",
            "Epoch 75/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0474 - accuracy: 0.9851\n",
            "Epoch 76/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0518 - accuracy: 0.9837\n",
            "Epoch 77/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0486 - accuracy: 0.9849\n",
            "Epoch 78/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0489 - accuracy: 0.9866\n",
            "Epoch 79/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0478 - accuracy: 0.9863\n",
            "Epoch 80/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0452 - accuracy: 0.9857\n",
            "Epoch 81/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0464 - accuracy: 0.9865\n",
            "Epoch 82/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0486 - accuracy: 0.9845\n",
            "Epoch 83/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0535 - accuracy: 0.9831\n",
            "Epoch 84/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0447 - accuracy: 0.9863\n",
            "Epoch 85/200\n",
            "6496/6496 [==============================] - 0s 10us/step - loss: 0.0441 - accuracy: 0.9863\n",
            "Epoch 86/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0429 - accuracy: 0.9877\n",
            "Epoch 87/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0560 - accuracy: 0.9832\n",
            "Epoch 88/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0462 - accuracy: 0.9858\n",
            "Epoch 89/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0462 - accuracy: 0.9851\n",
            "Epoch 90/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0470 - accuracy: 0.9841\n",
            "Epoch 91/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0466 - accuracy: 0.9841\n",
            "Epoch 92/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0441 - accuracy: 0.9861\n",
            "Epoch 93/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0416 - accuracy: 0.9872\n",
            "Epoch 94/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0467 - accuracy: 0.9851\n",
            "Epoch 95/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0433 - accuracy: 0.9869\n",
            "Epoch 96/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0462 - accuracy: 0.9860\n",
            "Epoch 97/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0423 - accuracy: 0.9871\n",
            "Epoch 98/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0483 - accuracy: 0.9848\n",
            "Epoch 99/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0442 - accuracy: 0.9858\n",
            "Epoch 100/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0465 - accuracy: 0.9846\n",
            "Epoch 101/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0441 - accuracy: 0.9855\n",
            "Epoch 102/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0565 - accuracy: 0.9818\n",
            "Epoch 103/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0517 - accuracy: 0.9831\n",
            "Epoch 104/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0414 - accuracy: 0.9880\n",
            "Epoch 105/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0471 - accuracy: 0.9863\n",
            "Epoch 106/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0550 - accuracy: 0.9826\n",
            "Epoch 107/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0560 - accuracy: 0.9811\n",
            "Epoch 108/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0435 - accuracy: 0.9861\n",
            "Epoch 109/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0411 - accuracy: 0.9858\n",
            "Epoch 110/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0413 - accuracy: 0.9874\n",
            "Epoch 111/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0412 - accuracy: 0.9878\n",
            "Epoch 112/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0415 - accuracy: 0.9858\n",
            "Epoch 113/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0434 - accuracy: 0.9877\n",
            "Epoch 114/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0443 - accuracy: 0.9855\n",
            "Epoch 115/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0459 - accuracy: 0.9858\n",
            "Epoch 116/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0451 - accuracy: 0.9852\n",
            "Epoch 117/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0499 - accuracy: 0.9831\n",
            "Epoch 118/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0429 - accuracy: 0.9874\n",
            "Epoch 119/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0415 - accuracy: 0.9875\n",
            "Epoch 120/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0397 - accuracy: 0.9880\n",
            "Epoch 121/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0401 - accuracy: 0.9888\n",
            "Epoch 122/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0401 - accuracy: 0.9885\n",
            "Epoch 123/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0395 - accuracy: 0.9885\n",
            "Epoch 124/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0441 - accuracy: 0.9869\n",
            "Epoch 125/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0395 - accuracy: 0.9880\n",
            "Epoch 126/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0446 - accuracy: 0.9868\n",
            "Epoch 127/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0428 - accuracy: 0.9875\n",
            "Epoch 128/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0424 - accuracy: 0.9874\n",
            "Epoch 129/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0445 - accuracy: 0.9872\n",
            "Epoch 130/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0434 - accuracy: 0.9852\n",
            "Epoch 131/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0438 - accuracy: 0.9858\n",
            "Epoch 132/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0517 - accuracy: 0.9848\n",
            "Epoch 133/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0474 - accuracy: 0.9848\n",
            "Epoch 134/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0421 - accuracy: 0.9866\n",
            "Epoch 135/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0407 - accuracy: 0.9868\n",
            "Epoch 136/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.0398 - accuracy: 0.9868\n",
            "Epoch 137/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0408 - accuracy: 0.9869\n",
            "Epoch 138/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0395 - accuracy: 0.9883\n",
            "Epoch 139/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0398 - accuracy: 0.9877\n",
            "Epoch 140/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0383 - accuracy: 0.9892\n",
            "Epoch 141/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0367 - accuracy: 0.9886\n",
            "Epoch 142/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0368 - accuracy: 0.9888\n",
            "Epoch 143/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.0394 - accuracy: 0.9878\n",
            "Epoch 144/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0354 - accuracy: 0.9894\n",
            "Epoch 145/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0435 - accuracy: 0.9869\n",
            "Epoch 146/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0429 - accuracy: 0.9869\n",
            "Epoch 147/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0408 - accuracy: 0.9872\n",
            "Epoch 148/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0421 - accuracy: 0.9863\n",
            "Epoch 149/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0422 - accuracy: 0.9877\n",
            "Epoch 150/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0407 - accuracy: 0.9861\n",
            "Epoch 151/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0394 - accuracy: 0.9880\n",
            "Epoch 152/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0432 - accuracy: 0.9861\n",
            "Epoch 153/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0383 - accuracy: 0.9889\n",
            "Epoch 154/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0454 - accuracy: 0.9848\n",
            "Epoch 155/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0384 - accuracy: 0.9880\n",
            "Epoch 156/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0380 - accuracy: 0.9881\n",
            "Epoch 157/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0434 - accuracy: 0.9868\n",
            "Epoch 158/200\n",
            "6496/6496 [==============================] - 0s 9us/step - loss: 0.0338 - accuracy: 0.9894\n",
            "Epoch 159/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0391 - accuracy: 0.9883\n",
            "Epoch 160/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0365 - accuracy: 0.9888\n",
            "Epoch 161/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0387 - accuracy: 0.9871\n",
            "Epoch 162/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0383 - accuracy: 0.9894\n",
            "Epoch 163/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0364 - accuracy: 0.9883\n",
            "Epoch 164/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0354 - accuracy: 0.9894\n",
            "Epoch 165/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0452 - accuracy: 0.9865\n",
            "Epoch 166/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0379 - accuracy: 0.9889\n",
            "Epoch 167/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0356 - accuracy: 0.9892\n",
            "Epoch 168/200\n",
            "6496/6496 [==============================] - 0s 7us/step - loss: 0.0380 - accuracy: 0.9880\n",
            "Epoch 169/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0387 - accuracy: 0.9877\n",
            "Epoch 170/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0370 - accuracy: 0.9886\n",
            "Epoch 171/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0346 - accuracy: 0.9903\n",
            "Epoch 172/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0428 - accuracy: 0.9857\n",
            "Epoch 173/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0399 - accuracy: 0.9889\n",
            "Epoch 174/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0374 - accuracy: 0.9883\n",
            "Epoch 175/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0336 - accuracy: 0.9903\n",
            "Epoch 176/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0356 - accuracy: 0.9897\n",
            "Epoch 177/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0350 - accuracy: 0.9889\n",
            "Epoch 178/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0342 - accuracy: 0.9891\n",
            "Epoch 179/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0446 - accuracy: 0.9855\n",
            "Epoch 180/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0346 - accuracy: 0.9889\n",
            "Epoch 181/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0368 - accuracy: 0.9888\n",
            "Epoch 182/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0369 - accuracy: 0.9885\n",
            "Epoch 183/200\n",
            "6496/6496 [==============================] - 0s 10us/step - loss: 0.0390 - accuracy: 0.9869\n",
            "Epoch 184/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0366 - accuracy: 0.9895\n",
            "Epoch 185/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0365 - accuracy: 0.9883\n",
            "Epoch 186/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0357 - accuracy: 0.9897\n",
            "Epoch 187/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0349 - accuracy: 0.9889\n",
            "Epoch 188/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0375 - accuracy: 0.9886\n",
            "Epoch 189/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0417 - accuracy: 0.9874\n",
            "Epoch 190/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0415 - accuracy: 0.9874\n",
            "Epoch 191/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0396 - accuracy: 0.9875\n",
            "Epoch 192/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0406 - accuracy: 0.9874\n",
            "Epoch 193/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0466 - accuracy: 0.9858\n",
            "Epoch 194/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0329 - accuracy: 0.9905\n",
            "Epoch 195/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0319 - accuracy: 0.9920\n",
            "Epoch 196/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0334 - accuracy: 0.9905\n",
            "Epoch 197/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0462 - accuracy: 0.9851\n",
            "Epoch 198/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0413 - accuracy: 0.9875\n",
            "Epoch 199/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0364 - accuracy: 0.9901\n",
            "Epoch 200/200\n",
            "6496/6496 [==============================] - 0s 8us/step - loss: 0.0423 - accuracy: 0.9866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8cd7c6a438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhQQG070_kFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "MODEL_DIR = './model/'\n",
        "if not os.path.exists(MODEL_DIR) :\n",
        "  os.mkdir(MODEL_DIR)\n",
        "\n",
        "modelpath = \"./model/{epoch:02d}-{val-loss:4f}.hdf5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1f5_fp4AkYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checkpointer : 모니터할 값을 지정 \n",
        "# 테스트 오차는 케라스 내부에서 val_loss로 기록됨\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath , monitor='val_loss' , verbose =1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhjVixhUA39Z",
        "colab_type": "code",
        "outputId": "16e0ea01-9a4a-4651-a125-5a255e61ff8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "model.fit(x,y,validation_split=0.2, epochs=200,batch_size=200,verbose=0,callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-620ce9c264cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val-loss'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq51VvQkBB8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping \n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss' , patience = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_XK7GWuB1Gt",
        "colab_type": "code",
        "outputId": "dbb49f73-2453-4683-ebb2-c800ac5a8436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x,y,validation_split=0.33, epochs=2000, batch_size=500 , callbacks=[early_stopping_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4352 samples, validate on 2144 samples\n",
            "Epoch 1/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0236 - accuracy: 0.9938 - val_loss: 0.0349 - val_accuracy: 0.9925\n",
            "Epoch 2/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0235 - accuracy: 0.9931 - val_loss: 0.0354 - val_accuracy: 0.9902\n",
            "Epoch 3/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0238 - accuracy: 0.9933 - val_loss: 0.0412 - val_accuracy: 0.9883\n",
            "Epoch 4/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0257 - accuracy: 0.9926 - val_loss: 0.0370 - val_accuracy: 0.9907\n",
            "Epoch 5/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0239 - accuracy: 0.9936 - val_loss: 0.0349 - val_accuracy: 0.9897\n",
            "Epoch 6/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0235 - accuracy: 0.9938 - val_loss: 0.0422 - val_accuracy: 0.9916\n",
            "Epoch 7/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.0389 - val_accuracy: 0.9921\n",
            "Epoch 8/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0239 - accuracy: 0.9938 - val_loss: 0.0382 - val_accuracy: 0.9902\n",
            "Epoch 9/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0237 - accuracy: 0.9938 - val_loss: 0.0354 - val_accuracy: 0.9921\n",
            "Epoch 10/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0240 - accuracy: 0.9938 - val_loss: 0.0378 - val_accuracy: 0.9902\n",
            "Epoch 11/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.0379 - val_accuracy: 0.9897\n",
            "Epoch 12/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0234 - accuracy: 0.9931 - val_loss: 0.0353 - val_accuracy: 0.9907\n",
            "Epoch 13/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0240 - accuracy: 0.9936 - val_loss: 0.0365 - val_accuracy: 0.9921\n",
            "Epoch 14/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0236 - accuracy: 0.9933 - val_loss: 0.0368 - val_accuracy: 0.9916\n",
            "Epoch 15/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0253 - accuracy: 0.9933 - val_loss: 0.0394 - val_accuracy: 0.9911\n",
            "Epoch 16/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0259 - accuracy: 0.9924 - val_loss: 0.0367 - val_accuracy: 0.9916\n",
            "Epoch 17/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0236 - accuracy: 0.9940 - val_loss: 0.0377 - val_accuracy: 0.9897\n",
            "Epoch 18/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0230 - accuracy: 0.9938 - val_loss: 0.0373 - val_accuracy: 0.9902\n",
            "Epoch 19/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0229 - accuracy: 0.9938 - val_loss: 0.0382 - val_accuracy: 0.9911\n",
            "Epoch 20/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0227 - accuracy: 0.9938 - val_loss: 0.0358 - val_accuracy: 0.9902\n",
            "Epoch 21/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0230 - accuracy: 0.9936 - val_loss: 0.0392 - val_accuracy: 0.9893\n",
            "Epoch 22/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0235 - accuracy: 0.9933 - val_loss: 0.0414 - val_accuracy: 0.9893\n",
            "Epoch 23/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0251 - accuracy: 0.9933 - val_loss: 0.0396 - val_accuracy: 0.9902\n",
            "Epoch 24/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0256 - accuracy: 0.9926 - val_loss: 0.0370 - val_accuracy: 0.9916\n",
            "Epoch 25/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0241 - accuracy: 0.9933 - val_loss: 0.0363 - val_accuracy: 0.9916\n",
            "Epoch 26/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0238 - accuracy: 0.9940 - val_loss: 0.0365 - val_accuracy: 0.9907\n",
            "Epoch 27/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.0366 - val_accuracy: 0.9916\n",
            "Epoch 28/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0252 - accuracy: 0.9926 - val_loss: 0.0426 - val_accuracy: 0.9893\n",
            "Epoch 29/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0313 - accuracy: 0.9887 - val_loss: 0.0410 - val_accuracy: 0.9930\n",
            "Epoch 30/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0233 - accuracy: 0.9936 - val_loss: 0.0360 - val_accuracy: 0.9897\n",
            "Epoch 31/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0234 - accuracy: 0.9933 - val_loss: 0.0390 - val_accuracy: 0.9921\n",
            "Epoch 32/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0233 - accuracy: 0.9938 - val_loss: 0.0368 - val_accuracy: 0.9921\n",
            "Epoch 33/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0230 - accuracy: 0.9936 - val_loss: 0.0373 - val_accuracy: 0.9911\n",
            "Epoch 34/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.0407 - val_accuracy: 0.9893\n",
            "Epoch 35/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0228 - accuracy: 0.9940 - val_loss: 0.0371 - val_accuracy: 0.9916\n",
            "Epoch 36/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0248 - accuracy: 0.9931 - val_loss: 0.0351 - val_accuracy: 0.9902\n",
            "Epoch 37/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0260 - accuracy: 0.9929 - val_loss: 0.0408 - val_accuracy: 0.9897\n",
            "Epoch 38/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.0426 - val_accuracy: 0.9902\n",
            "Epoch 39/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0298 - accuracy: 0.9917 - val_loss: 0.0431 - val_accuracy: 0.9902\n",
            "Epoch 40/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0269 - accuracy: 0.9922 - val_loss: 0.0416 - val_accuracy: 0.9897\n",
            "Epoch 41/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0248 - accuracy: 0.9924 - val_loss: 0.0377 - val_accuracy: 0.9897\n",
            "Epoch 42/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0227 - accuracy: 0.9938 - val_loss: 0.0400 - val_accuracy: 0.9925\n",
            "Epoch 43/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0247 - accuracy: 0.9931 - val_loss: 0.0384 - val_accuracy: 0.9921\n",
            "Epoch 44/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0264 - accuracy: 0.9926 - val_loss: 0.0387 - val_accuracy: 0.9902\n",
            "Epoch 45/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0290 - accuracy: 0.9913 - val_loss: 0.0520 - val_accuracy: 0.9883\n",
            "Epoch 46/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0324 - accuracy: 0.9890 - val_loss: 0.0479 - val_accuracy: 0.9897\n",
            "Epoch 47/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0259 - accuracy: 0.9922 - val_loss: 0.0428 - val_accuracy: 0.9897\n",
            "Epoch 48/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0254 - accuracy: 0.9929 - val_loss: 0.0403 - val_accuracy: 0.9921\n",
            "Epoch 49/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0231 - accuracy: 0.9940 - val_loss: 0.0380 - val_accuracy: 0.9902\n",
            "Epoch 50/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0222 - accuracy: 0.9943 - val_loss: 0.0394 - val_accuracy: 0.9916\n",
            "Epoch 51/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0235 - accuracy: 0.9938 - val_loss: 0.0461 - val_accuracy: 0.9888\n",
            "Epoch 52/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0248 - accuracy: 0.9926 - val_loss: 0.0411 - val_accuracy: 0.9907\n",
            "Epoch 53/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0260 - accuracy: 0.9913 - val_loss: 0.0478 - val_accuracy: 0.9860\n",
            "Epoch 54/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.0372 - val_accuracy: 0.9916\n",
            "Epoch 55/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0231 - accuracy: 0.9938 - val_loss: 0.0389 - val_accuracy: 0.9907\n",
            "Epoch 56/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0229 - accuracy: 0.9945 - val_loss: 0.0389 - val_accuracy: 0.9907\n",
            "Epoch 57/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0215 - accuracy: 0.9940 - val_loss: 0.0400 - val_accuracy: 0.9897\n",
            "Epoch 58/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0244 - accuracy: 0.9931 - val_loss: 0.0609 - val_accuracy: 0.9841\n",
            "Epoch 59/2000\n",
            "4352/4352 [==============================] - 0s 8us/step - loss: 0.0328 - accuracy: 0.9890 - val_loss: 0.0398 - val_accuracy: 0.9902\n",
            "Epoch 60/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0469 - accuracy: 0.9853 - val_loss: 0.0468 - val_accuracy: 0.9865\n",
            "Epoch 61/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0324 - accuracy: 0.9897 - val_loss: 0.0381 - val_accuracy: 0.9916\n",
            "Epoch 62/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0279 - accuracy: 0.9920 - val_loss: 0.0409 - val_accuracy: 0.9921\n",
            "Epoch 63/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0253 - accuracy: 0.9931 - val_loss: 0.0416 - val_accuracy: 0.9907\n",
            "Epoch 64/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0243 - accuracy: 0.9924 - val_loss: 0.0410 - val_accuracy: 0.9897\n",
            "Epoch 65/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0245 - accuracy: 0.9936 - val_loss: 0.0410 - val_accuracy: 0.9893\n",
            "Epoch 66/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0224 - accuracy: 0.9940 - val_loss: 0.0381 - val_accuracy: 0.9916\n",
            "Epoch 67/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0221 - accuracy: 0.9938 - val_loss: 0.0452 - val_accuracy: 0.9902\n",
            "Epoch 68/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0226 - accuracy: 0.9938 - val_loss: 0.0387 - val_accuracy: 0.9907\n",
            "Epoch 69/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0216 - accuracy: 0.9943 - val_loss: 0.0427 - val_accuracy: 0.9897\n",
            "Epoch 70/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0235 - accuracy: 0.9936 - val_loss: 0.0520 - val_accuracy: 0.9874\n",
            "Epoch 71/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0403 - accuracy: 0.9878 - val_loss: 0.0540 - val_accuracy: 0.9879\n",
            "Epoch 72/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0356 - accuracy: 0.9881 - val_loss: 0.0374 - val_accuracy: 0.9897\n",
            "Epoch 73/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0258 - accuracy: 0.9924 - val_loss: 0.0446 - val_accuracy: 0.9921\n",
            "Epoch 74/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0261 - accuracy: 0.9924 - val_loss: 0.0396 - val_accuracy: 0.9897\n",
            "Epoch 75/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0244 - accuracy: 0.9936 - val_loss: 0.0392 - val_accuracy: 0.9921\n",
            "Epoch 76/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0234 - accuracy: 0.9938 - val_loss: 0.0383 - val_accuracy: 0.9916\n",
            "Epoch 77/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.0388 - val_accuracy: 0.9925\n",
            "Epoch 78/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0236 - accuracy: 0.9943 - val_loss: 0.0397 - val_accuracy: 0.9911\n",
            "Epoch 79/2000\n",
            "4352/4352 [==============================] - 0s 5us/step - loss: 0.0253 - accuracy: 0.9931 - val_loss: 0.0407 - val_accuracy: 0.9902\n",
            "Epoch 80/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.0423 - val_accuracy: 0.9921\n",
            "Epoch 81/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0226 - accuracy: 0.9933 - val_loss: 0.0389 - val_accuracy: 0.9911\n",
            "Epoch 82/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0227 - accuracy: 0.9940 - val_loss: 0.0433 - val_accuracy: 0.9921\n",
            "Epoch 83/2000\n",
            "4352/4352 [==============================] - 0s 7us/step - loss: 0.0221 - accuracy: 0.9947 - val_loss: 0.0386 - val_accuracy: 0.9911\n",
            "Epoch 84/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0228 - accuracy: 0.9933 - val_loss: 0.0416 - val_accuracy: 0.9921\n",
            "Epoch 85/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 0.0386 - val_accuracy: 0.9916\n",
            "Epoch 86/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0222 - accuracy: 0.9943 - val_loss: 0.0413 - val_accuracy: 0.9907\n",
            "Epoch 87/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0225 - accuracy: 0.9933 - val_loss: 0.0410 - val_accuracy: 0.9907\n",
            "Epoch 88/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0218 - accuracy: 0.9936 - val_loss: 0.0388 - val_accuracy: 0.9907\n",
            "Epoch 89/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0220 - accuracy: 0.9943 - val_loss: 0.0412 - val_accuracy: 0.9930\n",
            "Epoch 90/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0221 - accuracy: 0.9940 - val_loss: 0.0399 - val_accuracy: 0.9930\n",
            "Epoch 91/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0214 - accuracy: 0.9945 - val_loss: 0.0411 - val_accuracy: 0.9897\n",
            "Epoch 92/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0222 - accuracy: 0.9943 - val_loss: 0.0413 - val_accuracy: 0.9925\n",
            "Epoch 93/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0223 - accuracy: 0.9936 - val_loss: 0.0395 - val_accuracy: 0.9921\n",
            "Epoch 94/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0233 - accuracy: 0.9933 - val_loss: 0.0426 - val_accuracy: 0.9911\n",
            "Epoch 95/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0212 - accuracy: 0.9952 - val_loss: 0.0413 - val_accuracy: 0.9916\n",
            "Epoch 96/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0228 - accuracy: 0.9933 - val_loss: 0.0426 - val_accuracy: 0.9925\n",
            "Epoch 97/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0221 - accuracy: 0.9938 - val_loss: 0.0414 - val_accuracy: 0.9897\n",
            "Epoch 98/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0226 - accuracy: 0.9936 - val_loss: 0.0430 - val_accuracy: 0.9921\n",
            "Epoch 99/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0216 - accuracy: 0.9943 - val_loss: 0.0445 - val_accuracy: 0.9902\n",
            "Epoch 100/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0214 - accuracy: 0.9938 - val_loss: 0.0431 - val_accuracy: 0.9916\n",
            "Epoch 101/2000\n",
            "4352/4352 [==============================] - 0s 6us/step - loss: 0.0218 - accuracy: 0.9940 - val_loss: 0.0462 - val_accuracy: 0.9902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8cd7cecef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l7GafHZDblN",
        "colab_type": "text"
      },
      "source": [
        "#  CNN의 Hello world - MNIST \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDweWOd3B_Gt",
        "colab_type": "code",
        "outputId": "62605aa4-2075-4c7a-bffb-211606204bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils # 원핫인코딩\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout , Flatten , Conv2D , MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_D4DZkZNBn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import os \n",
        "import tensorflow as tf "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ThBlRTnNJyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#seed 값 설정 \n",
        "seed = 0 \n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXnN0UOmNXs7",
        "colab_type": "text"
      },
      "source": [
        "- 데이터 불러오기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqGXowrqNJtu",
        "colab_type": "code",
        "outputId": "2ba72461-d5b3-4453-a8ea-d2e6dc0ca484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "(x_train ,  y_class_train) , (x_test , y_clsas_test) = mnist.load_data()\n",
        "\n",
        "plt.imshow(x_train[2] , cmap='Greys')\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s3wk_9LNJpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2차원 배열인 이미지의 픽셀 값을 1차원 배열로 바꿔줘야함 \n",
        "#reshape(총 샘플 수 , 1차원 속성 수 )\n",
        "#keras는 데이터를 0에서 1 사이의 값으로 변환하여 구동할때 최적의 성능을 보인다.\n",
        "# 따라서 정규화 해야한다. (0~1)\n",
        "x_train = x_train.reshape(x_train.shape[0] , 28,28,1).astype('float32') / 255 \n",
        "x_test = x_test.reshape(x_test.shape[0] , 28,28,1).astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCH5NSRzNJey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#y class를 원핫 인코딩해야함 \n",
        "y_train = np_utils.to_categorical(y_class_train,10)\n",
        "y_test = np_utils.to_categorical(y_clsas_test,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgDCN3-RN82f",
        "colab_type": "text"
      },
      "source": [
        "- 컨볼루션 신경망 설정 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc5Wt7U6DiyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 입력값이 784개 / 은닉층이 512개 / 출력이 10개 \n",
        "model = Sequential()\n",
        "#input_shape = (행,열,색상 혹은 흑백 )\n",
        "model.add(Conv2D(32, kernel_size= (3,3) , input_shape = (28,28,1) , activation='relu'))\n",
        "model.add(Conv2D(64,(3,3) , activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size= 2)) # 풀링 창 , 2로 정하면 절반으로 줄어듬 \n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128 ,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10,activation='softmax')) # 출력이 10개 \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCRAAe_7D3fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy' ,optimizer='adam' , metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O54fI2AnPkdb",
        "colab_type": "text"
      },
      "source": [
        "- 모델 최적화 설정 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xu8amjVD77V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = './model/'\n",
        "if not os.path.exists(MODEL_DIR) :\n",
        "  os.mkdir(MODEL_DIR)\n",
        "model_path = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "\n",
        "  # 가장 좋은 모델만 저장해라 \n",
        "check_pointer = ModelCheckpoint(filepath=model_path,monitor='val_loss' , verbose =1 , save_best_only=True) \n",
        "early_stopping_callback_ = EarlyStopping(monitor='val_loss' , patience=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUHuP02-QdBw",
        "colab_type": "text"
      },
      "source": [
        "- 모델의 실행 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1PSvnNLEj2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train , y_train, validation_data=(x_test,y_test) ,epochs= 50 , batch_size=200 , verbose=0, callbacks =[early_stopping_callback_])\n",
        "print(\"\\n Test Accuracy : %.4f\" % (model.evaluate(x_test,y_test)[1]))                  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgnWn4B2Rq3H",
        "colab_type": "text"
      },
      "source": [
        "- 오차 그래프로 표현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpCNeSIxE8_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 테스트셋의 오차 \n",
        "y_vloss = history.history['val_loss']\n",
        "\n",
        "#학습셋의 오차 \n",
        "y_loss = history.history['loss']\n",
        "\n",
        "## cf. acc : 학습 정확도 \n",
        "## val_acc : 테스트셋 정확도 \n",
        "## loss  : 학습셋 오차 \n",
        "## val_loss  : 테스트셋 오차 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT0N2IzxFgzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 그래프로 표현 \n",
        "x_len = np.arange(len(y_loss))\n",
        "plt.plt(x_len , y_vloss , marker=',' , c='red' , label='TestSet_loss')\n",
        "plt.plt(x_len , y_loss , marker=',' , c='blue' , label='TrainSet_loss')\n",
        "\n",
        "#그래프에 그리드르 주고 레이블을 표시 \n",
        "plt.legend(loc ='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMuIbikNF80r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPY6J8DOIkYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}